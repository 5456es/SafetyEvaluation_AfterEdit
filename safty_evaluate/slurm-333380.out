Model Path:  /home/k/kduan/szn_workspace/hf_cache_soc/llama-2-7b-chat-hf/
Data Path:  /home/k/kduan/szn_workspace/SafetyEvaluation_AfterEdit/data/Eval_data/merged_deduplicated_data.json
Output Path:  ./llama/baseline
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [10:09<10:09, 609.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [13:04<00:00, 353.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [13:04<00:00, 392.05s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
0  to  50
generating!
50  to  100
generating!
100  to  150
generating!
150  to  200
generating!
200  to  250
generating!
250  to  300
generating!
300  to  350
generating!
350  to  400
generating!
400  to  450
generating!
450  to  500
generating!
500  to  550
generating!
ZsRE_1 Done
