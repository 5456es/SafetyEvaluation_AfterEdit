{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.45.2\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003682851791381836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e4e44f2f934f1693dc75c28d285d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "TOKEN='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'\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_path='/home/bizon/zns_workspace/24_09_Evaluation/hugging_cache/mistral-7b-instruct-v0.3/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db'\n",
    "### print transformers version\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "  #  \"/home/bizon/zns_workspace/Safety_Evaluation_After_Edit/results/ROME/20241011_1525/edited_model\",\n",
    "    device_map=\"auto\",\n",
    "    # Double quantization\n",
    "    # quantization_config=BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    # ),\n",
    "   #  use_auth_token=TOKEN,\n",
    ")\n",
    "\n",
    "# ~/szn_workspace/Edit_Evaluation/hugging_cache/Llama-2-7b-hf/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,     3,  7294,  1117,  1214,  9666,  1938,  1208, 29572, 27075,\n",
      "          2480,  3853,  1633, 29491, 27075,  1065, 29473, 29550,  3853,  8303,\n",
      "         17057, 29561, 29473]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['<s>', '[INST]', 'Who', 'is', 'k', 'obe', 'br', 'ant', '?', 'Answer', 'three', 'words', 'only', '.', 'Answer', 'in', '', '5', 'words', '[\\\\', 'INST', ']', '']\n",
      "torch.Size([1, 23, 32768])\n",
      "answers: [23246, 29505, 1117, 1040, 1037, 1055, 1411, 1072, 781, 29515, 4992, 29491, 29491, 781, 29515, 4657, 29538, 7999, 1210, 29561, 13506, 26026, 29552]\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/miniconda3/envs/mistral/lib/python3.9/site-packages/transformers/generation/utils.py:1934: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is kobe brant? Answer three words only. Answer in 5 words[\\INST] 6-time NBA champion.\n",
      "\n",
      "Kobe Bryant was a professional basketball player who played for the Los Angeles Lakers in the NBA. He is known for his exceptional skills, work ethic, and competitive spirit. He passed away in a helicopter crash in 2020. [Instead of three words, I provided five words to give a brief overview of who Kobe Bryant was.]</s>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "prompt=tokenizer(\n",
    "    ['<s>[INST] Who is kobe brant? Answer three words only. Answer in 5 words [\\INST] '],\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False\n",
    ")\n",
    "print(prompt)\n",
    "print([tokenizer.decode(tok_id) for tok_id in (prompt[\"input_ids\"][0])])\n",
    "output=model(**prompt)\n",
    "print(output.logits.size())\n",
    "answers = torch.argmax(output.logits, dim=-1).squeeze().detach().cpu().numpy().tolist()\n",
    "print(f\"answers: {answers}\")\n",
    "\n",
    "# print([tokenizer.decode(answer) for answer in answers])\n",
    "# print([tokenizer.decode(original) for original in prompt['input_ids'][0]])\n",
    "print('here')\n",
    "# print((model.generate(**prompt,max_new_tokens=100)))\n",
    "\n",
    "print(tokenizer.decode(model.generate(**prompt,max_new_tokens=100)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19838, 424, 399], [19838, 3297, 286], [19838, 29911, 29876], [19838, 1817, 274], [19838, 3297, 289]]\n",
      "[[1, 399, 405], [1, 298, 288], [1, 474, 274], [1, 274, 260], [1, 298, 318]]\n",
      "hi\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/miniconda3/envs/EasyEdit/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/bizon/miniconda3/envs/EasyEdit/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "prompt=\"Which family does Epaspidoceras belong to?\"\n",
    "target_new='Noctu'\n",
    "def slice_list(matrix,start_indices,left):\n",
    "    if isinstance(matrix[0], list):\n",
    "        if left:\n",
    "            return [row[start_index-1:-1] for row, start_index in zip(matrix, start_indices)]\n",
    "        else:\n",
    "            return [row[start_index:] for row, start_index in zip(matrix, start_indices)]\n",
    "    else:\n",
    "        if left:\n",
    "            return matrix[start_indices[0]-1:-1]\n",
    "        else:\n",
    "            return matrix[start_indices[0]:]\n",
    "def test_prediction_acc(model, tok, prompts, targets, device=0, locality=False, vanilla_generation=False):\n",
    "    tok.pad_token = tok.eos_token\n",
    "    prompt_target = [prompt + ' ' + target for prompt, target in zip(prompts,targets)]\n",
    "    max_prompt_len = max([len(tok.encode(_)) for _ in prompt_target]) + 1\n",
    "    prompt_target_tok = tok(\n",
    "        prompt_target,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=20,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(f\"cuda:{device}\")\n",
    "    \n",
    "    prompt_tok = tok(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=20,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_tok['input_ids']]\n",
    "    num_pad_toks = [int((i == tok.pad_token_id).sum()) for i in prompt_target_tok['input_ids'].cpu()]\n",
    "    prompt_len = [x+y for x,y in zip(num_pad_toks,num_prompt_toks)]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**prompt_target_tok)\n",
    "        if type(outputs) is torch.Tensor:\n",
    "            logits = outputs\n",
    "        else:\n",
    "            logits = outputs.logits\n",
    "        answers = torch.argmax(logits, dim=-1).squeeze().detach().cpu().numpy().tolist()\n",
    "        labels = prompt_target_tok['input_ids'].squeeze().detach().cpu().numpy().tolist()\n",
    "        print(answers)\n",
    "        print(labels)\n",
    "        answers = slice_list(answers,prompt_len,left=True)\n",
    "        labels = slice_list(labels,prompt_len,left=False)\n",
    "        if locality:\n",
    "            return answers if type(answers[0]) is list else [answers,]\n",
    "        print(\"hi\")\n",
    "        if isinstance(answers[0], list):\n",
    "            res = []\n",
    "            for ans,label in zip(answers,labels):\n",
    "                temp_acc = np.mean(np.equal(ans, label))\n",
    "                if np.isnan(temp_acc):\n",
    "                    continue\n",
    "                res.append(temp_acc)\n",
    "            return res\n",
    "        else:\n",
    "            return [np.mean(np.equal(answers, labels))]\n",
    "print(test_prediction_acc(model,tokenizer,prompt,target_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,     3,  2592,  1117,  1342, 16788,  2844,  3255, 29572,     4,\n",
      "          5441, 29493,  1083, 29510, 29487,  4216, 11241,  1066,  1032,  1947,\n",
      "         12090, 20729,  1070,  6906, 24366, 19110, 29491,  1429, 14401,  1544,\n",
      "          1040,  1871,  4326,  1070,  1454, 17712, 16405,  1191,  1066,  6449,\n",
      "          1083, 29510, 29487, 13966,  1350,  1065,  1040,  6900, 29576,     2,\n",
      "             3,  3146,  1136,  1274,  1761,  7904,  1632, 21884, 29572,     4,\n",
      "          5441, 29493,  1083, 29510, 29487,  4216, 11241,  1066,  1032,  1947,\n",
      "         12090, 20729,  1070,  6906, 24366, 19110, 29491,  1429, 14401,  1544,\n",
      "          1040,  1871,  4326,  1070,  1454, 17712, 16405,  1191,  1066,  6449,\n",
      "          1083, 29510, 29487, 13966,  1350,  1065,  1040,  6900, 29576,     2]])\n",
      "<s>[INST] What is your favourite condiment?[/INST] Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>[INST] Do you have mayonnaise recipes?[/INST] Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Assuming you have already imported OpenAI and configured the client\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(base_url=\"https://api.novita.ai/v3/openai\", api_key=\"<YOUR Novita AI API Key>\")\n",
    "\n",
    "# Define your model and tokenizer\n",
    "model_name =  '/home/bizon/zns_workspace/24_09_Evaluation/hugging_cache/mistral-7b-instruct-v0.3/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "]\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "print(encodeds)\n",
    "\n",
    "print(tokenizer.decode(encodeds[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
