{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.45.2\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0049288272857666016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0c7aada38b4a73bb9c9b20a6056c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TOKEN='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'\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "### print transformers version\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    " '/home/bizon/zns_workspace/24_09_Evaluation/hugging_cache/mistral-7b-instruct-v0.3/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/'\n",
    "    \n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "'/home/bizon/zns_workspace/Safety_Evaluation_After_Edit/results/ROME/mistral-7b-v0.1/20241011_2003/edited_model',\n",
    "  #  \"/home/bizon/zns_workspace/Safety_Evaluation_After_Edit/results/ROME/20241011_1525/edited_model\",\n",
    "    device_map=\"auto\",\n",
    "    # Double quantization\n",
    "    # quantization_config=BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    # ),\n",
    "   #  use_auth_token=TOKEN,\n",
    ")\n",
    "\n",
    "# ~/szn_workspace/Edit_Evaluation/hugging_cache/Llama-2-7b-hf/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,  1183,  2773,  1070,  1040, 15478,  1061, 10275,  1170, 12002,\n",
      "         18595,  1066]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 12, 32768])\n",
      "[23246, 1675, 1070, 1032, 4677, 7327, 1106, 29478, 1082, 1117, 1066, 1040]\n",
      "['Question', 'first', 'of', 'a', 'late', 'stein', 'ch', 'i', 'am', 'is', 'to', 'the']\n",
      "['<s>', 'The', 'family', 'of', 'the', 'Ep', 'as', 'pid', 'oc', 'eras', 'belongs', 'to']\n",
      "here\n",
      "<s> The family of the Epaspidoceras belongs to the Noctuidae, also known as owlet moths.\n",
      "\n",
      "#### Identification\n",
      "\n",
      "The Epaspidoc\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "prompt=tokenizer(\n",
    "    ['The family of the Epaspidoceras belongs to'],\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(prompt)\n",
    "output=model(**prompt)\n",
    "print(output.logits.size())\n",
    "answers = torch.argmax(output.logits, dim=-1).squeeze().detach().cpu().numpy().tolist()\n",
    "print(answers)\n",
    "\n",
    "print([tokenizer.decode(answer) for answer in answers])\n",
    "print([tokenizer.decode(original) for original in prompt['input_ids'][0]])\n",
    "print('here')\n",
    "print(tokenizer.decode(model.generate(**prompt,max_length=40)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19838, 424, 399], [19838, 3297, 286], [19838, 29911, 29876], [19838, 1817, 274], [19838, 3297, 289]]\n",
      "[[1, 399, 405], [1, 298, 288], [1, 474, 274], [1, 274, 260], [1, 298, 318]]\n",
      "hi\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/miniconda3/envs/EasyEdit/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/bizon/miniconda3/envs/EasyEdit/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "prompt=\"Which family does Epaspidoceras belong to?\"\n",
    "target_new='Noctu'\n",
    "def slice_list(matrix,start_indices,left):\n",
    "    if isinstance(matrix[0], list):\n",
    "        if left:\n",
    "            return [row[start_index-1:-1] for row, start_index in zip(matrix, start_indices)]\n",
    "        else:\n",
    "            return [row[start_index:] for row, start_index in zip(matrix, start_indices)]\n",
    "    else:\n",
    "        if left:\n",
    "            return matrix[start_indices[0]-1:-1]\n",
    "        else:\n",
    "            return matrix[start_indices[0]:]\n",
    "def test_prediction_acc(model, tok, prompts, targets, device=0, locality=False, vanilla_generation=False):\n",
    "    tok.pad_token = tok.eos_token\n",
    "    prompt_target = [prompt + ' ' + target for prompt, target in zip(prompts,targets)]\n",
    "    max_prompt_len = max([len(tok.encode(_)) for _ in prompt_target]) + 1\n",
    "    prompt_target_tok = tok(\n",
    "        prompt_target,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=20,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(f\"cuda:{device}\")\n",
    "    \n",
    "    prompt_tok = tok(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=20,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_tok['input_ids']]\n",
    "    num_pad_toks = [int((i == tok.pad_token_id).sum()) for i in prompt_target_tok['input_ids'].cpu()]\n",
    "    prompt_len = [x+y for x,y in zip(num_pad_toks,num_prompt_toks)]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**prompt_target_tok)\n",
    "        if type(outputs) is torch.Tensor:\n",
    "            logits = outputs\n",
    "        else:\n",
    "            logits = outputs.logits\n",
    "        answers = torch.argmax(logits, dim=-1).squeeze().detach().cpu().numpy().tolist()\n",
    "        labels = prompt_target_tok['input_ids'].squeeze().detach().cpu().numpy().tolist()\n",
    "        print(answers)\n",
    "        print(labels)\n",
    "        answers = slice_list(answers,prompt_len,left=True)\n",
    "        labels = slice_list(labels,prompt_len,left=False)\n",
    "        if locality:\n",
    "            return answers if type(answers[0]) is list else [answers,]\n",
    "        print(\"hi\")\n",
    "        if isinstance(answers[0], list):\n",
    "            res = []\n",
    "            for ans,label in zip(answers,labels):\n",
    "                temp_acc = np.mean(np.equal(ans, label))\n",
    "                if np.isnan(temp_acc):\n",
    "                    continue\n",
    "                res.append(temp_acc)\n",
    "            return res\n",
    "        else:\n",
    "            return [np.mean(np.equal(answers, labels))]\n",
    "print(test_prediction_acc(model,tokenizer,prompt,target_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
